# -*- coding: utf-8 -*-
# === Serve-only RAG Benchmark: Ray Serve Deployments vs Rounds (version-agnostic) ===

from pathlib import Path
import gc, os, time, threading
from statistics import mean

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
import ray
from ray import serve

# -------------------- Config --------------------
os.environ.setdefault("HF_HOME", "/mnt/shared/hf-home")
os.environ.setdefault("HF_HUB_OFFLINE", "1")
os.environ.setdefault("TRANSFORMERS_OFFLINE", "1")
os.environ.setdefault("HF_DATASETS_OFFLINE", "1")

FAISS_DIR   = Path("/mnt/shared/faiss_index")
EMBED_MODEL = "/mnt/shared/models/ko-sroberta-multitask"
Q1_PATH     = Path("/mnt/shared/models/qwen25_3b")
Q2_PATH     = Path("/mnt/shared/models/qwen25_1_5b_instruct")

TOP_K = 4
MAX_NEW_TOKENS = 1024
ROUNDS = [1, 6, 12, 24, 48]

# -------------------- Utils --------------------
def load_vectorstore(faiss_dir: Path, embed_model: str) -> FAISS:
    emb = HuggingFaceEmbeddings(model_name=embed_model)
    return FAISS.load_local(str(faiss_dir), emb, allow_dangerous_deserialization=True)

def retrieve_context(vs: FAISS, query: str, k: int = TOP_K) -> str:
    docs = vs.similarity_search(query, k=k)
    return "\n\n".join(d.page_content.strip() for d in docs)

def load_local_llm(model_dir: Path):
    tok = AutoTokenizer.from_pretrained(str(model_dir))
    model = AutoModelForCausalLM.from_pretrained(
        str(model_dir),
        device_map="auto",
        dtype="auto",
    )
    return tok, model

# -------------------- Serve Deployment --------------------
@serve.deployment  # ë™ì‹œì„±ì€ ë‚´ë¶€ ì„¸ë§ˆí¬ì–´ë¡œ ê°•ì œ(ë²„ì „ ë¬´ê´€)
class ServeModelWorker:
    def __init__(self, model_dir: str):
        self.tok, self.model = load_local_llm(Path(model_dir))
        # ë ˆí”Œë¦¬ì¹´ë‹¹ ë™ì‹œ 1ìš”ì²­ ê°•ì œ (ë²„ì „ë³„ ì˜µì…˜ ì°¨ì´ íšŒí”¼)
        self._sem = threading.BoundedSemaphore(value=1)

    @torch.inference_mode()
    def generate_kpm(self, system_msg: str, user_msg: str, max_new_tokens: int = MAX_NEW_TOKENS):
        with self._sem:  # ë™ì‹œì²˜ë¦¬=1 ë³´ì¥
            messages = [
                {"role": "system", "content": system_msg},
                {"role": "user", "content": user_msg},
            ]
            input_ids = self.tok.apply_chat_template(
                messages, tokenize=True, add_generation_prompt=True, return_tensors="pt"
            ).to(self.model.device)

            streamer = TextIteratorStreamer(self.tok, skip_prompt=True, skip_special_tokens=True)
            t_send = time.perf_counter()

            gen_kwargs = dict(
                input_ids=input_ids,
                attention_mask=torch.ones_like(input_ids, dtype=torch.long),
                do_sample=False,
                max_new_tokens=max_new_tokens,
                eos_token_id=self.tok.eos_token_id,
                pad_token_id=self.tok.eos_token_id,
                streamer=streamer,
            )
            th = threading.Thread(target=self.model.generate, kwargs=gen_kwargs)
            th.start()

            first_token_time = None
            chunks = []
            for piece in streamer:
                if first_token_time is None:
                    first_token_time = time.perf_counter()
                chunks.append(piece)

            th.join()
            t_done = time.perf_counter()

            text = "".join(chunks).strip()
            out_tokens = self.tok.encode(text, add_special_tokens=False)

            return {
                "ttft": first_token_time - t_send if first_token_time else t_done - t_send,
                "latency": t_done - t_send,
                "n_tokens": len(out_tokens),
            }

def run_serve_benchmark(user_q1: str, user_q2: str, sys_msg: str):
    print("\n" + "="*80)
    print("               ğŸš€ STARTING BENCHMARK: Ray Serve (ONLY) ğŸš€")
    print("="*80 + "\n")

    available_gpus = int(ray.cluster_resources().get("GPU", 0))
    if available_gpus < 1:
        print("Error: No GPUs available.")
        return

    n_q1 = max(1, available_gpus // 2)
    n_q2 = max(0, available_gpus - n_q1)
    print(f"Total GPUs: {available_gpus}. Deploying {n_q1} replicas for Q1 and {n_q2} for Q2...")

    # Q1 ì•±
    q1_app = ServeModelWorker.options(
        name="q1_worker",
        num_replicas=n_q1,
        ray_actor_options={"num_gpus": 1},
    ).bind(model_dir=str(Q1_PATH))
    serve.run(q1_app, name="q1_app", route_prefix=None)

    # Q2 ì•±(ë ˆí”Œë¦¬ì¹´ 0ì´ë©´ ìƒëµ)
    if n_q2 > 0:
        q2_app = ServeModelWorker.options(
            name="q2_worker",
            num_replicas=n_q2,
            ray_actor_options={"num_gpus": 1},
        ).bind(model_dir=str(Q2_PATH))
        serve.run(q2_app, name="q2_app", route_prefix=None)

    # í•¸ë“¤ ì·¨ë“
    q1h = serve.get_deployment_handle("q1_worker", app_name="q1_app")
    q2h = serve.get_deployment_handle("q2_worker", app_name="q2_app") if n_q2 > 0 else None
    print("Serve handles are ready.")

    for n in ROUNDS:
        # -------------------- [PATCH] n=1ì—ì„œ Q1+Q2ë¥¼ ê°™ì€ ë¼ìš´ë“œì— í•¨ê»˜ ì‹¤í–‰ --------------------
        if n == 1:
            tasks = [("Q1", user_q1), ("Q2", user_q2)]
        else:
            tasks = [("Q1", user_q1) if i % 2 == 0 else ("Q2", user_q2) for i in range(n)]
        # ----------------------------------------------------------------------------------------

        t0 = time.perf_counter()
        futures = []
        for tag, um in tasks:
            # Q2 ë ˆí”Œë¦¬ì¹´ê°€ ì—†ìœ¼ë©´ ì•ˆì „í•˜ê²Œ Q1ë¡œ ë¼ìš°íŒ…(ì›ë˜ ì½”ë“œ ìœ ì§€)
            h = q1h if (tag == "Q1" or q2h is None) else q2h
            futures.append(h.generate_kpm.remote(sys_msg, um))
        results = [r.result() for r in futures]
        t1 = time.perf_counter()

        ttfts = [r["ttft"] for r in results if r["ttft"] is not None]
        lats  = [r["latency"] for r in results]
        toks  = sum(r["n_tokens"] for r in results)

        ttft_avg = mean(ttfts) if ttfts else 0.0
        lat_avg  = mean(lats) if lats else 0.0
        wall     = t1 - t0
        tput     = toks / wall if wall > 0 else 0.0

        print(f"\n[ROUND {n}]")
        print(f"  - Requests_in_round              : {len(results)}")  # ìš”ì²­ ìˆ˜ ëª…ì‹œ
        print(f"  - TTFT_avg_sec                   : {ttft_avg:.6f}")
        print(f"  - Latency_avg_sec (per request)  : {lat_avg:.6f}")
        print(f"  - Latency_total_sec (wall time)  : {wall:.6f}")
        print(f"  - Total_Throughput_tokens_per_sec: {tput:.6f}")

    serve.shutdown()
    gc.collect()
    print("\nâœ… Serve benchmark finished and Serve has been shut down.")

# -------------------- Main --------------------
def main():
    q1 = "ìƒê¸‰ìì˜ ë¶€ë‹¹í•œ ì§€ì‹œë¥¼ ë°›ì€ ê²½ìš° í•˜ê¸‰ìê°€ ì·¨í•  ìˆ˜ ìˆëŠ” ì ˆì°¨ë¥¼ ë‹¨ê³„ë³„ë¡œ ë‚˜ì—´í•´ì¤˜. ê·¸ë¦¬ê³  etri í–‰ë™ê°•ë ¹ì— ëª‡ ì¥ì— í•´ë‹¹í•˜ëŠ” ë‚´ìš©ì´ ë‚˜ì˜¤ëŠ” ì§€ ì•Œë ¤ì¤˜."
    q2 = "ì‚¬ì  ì´í•´ê´€ê³„ì'ì— í•´ë‹¹í•˜ëŠ” ì‚¬ëŒì€ ì´ ëª‡ ì¢…ë¥˜ì´ë©°, ê°ê° ëˆ„êµ¬ë¥¼ ì˜ë¯¸í•˜ëŠ”ì§€ ì •ë¦¬í•´ì¤˜."
    sys_msg = (
        "ë„ˆëŠ” 'ETRI ì„ì§ì› í–‰ë™ê°•ë ¹(etri.txt)'ë§Œì„ ê·¼ê±°ë¡œ ê°„ê²°í•˜ê³  ì •í™•íˆ ë‹µí•œë‹¤. "
        "ì¶œì²˜ê°€ ëª¨í˜¸í•˜ë©´ 'ë¬¸ì„œ ê·¼ê±° ë¶ˆì¶©ë¶„'ì´ë¼ê³  ëª…ì‹œí•œë‹¤. ê°€ëŠ¥í•œ ê²½ìš° ì¥/ì ˆ/ì¡°ë¥¼ í•¨ê»˜ í‘œê¸°í•œë‹¤."
    )

    print("Initializing Ray...")
    ray.init(address="auto", ignore_reinit_error=True)
    print("Ray connected successfully.")
    print("Ray version:", ray.__version__)

    print("Loading Vector Store and retrieving context...")
    vs = load_vectorstore(FAISS_DIR, EMBED_MODEL)
    ctx_q1 = retrieve_context(vs, q1, k=TOP_K)
    ctx_q2 = retrieve_context(vs, q2, k=TOP_K)
    user_q1 = f"[ì»¨í…ìŠ¤íŠ¸]\n{ctx_q1}\n\n[ì§ˆë¬¸]\n{q1}"
    user_q2 = f"[ì»¨í…ìŠ¤íŠ¸]\n{ctx_q2}\n\n[ì§ˆë¬¸]\n{q2}"
    print("Context retrieved.")

    run_serve_benchmark(user_q1, user_q2, sys_msg)

    ray.shutdown()
    print("\nâœ… Ray has been shut down.")

if __name__ == "__main__":
    main()
