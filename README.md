
ğŸš€ Rayâ€“vLLM ê¸°ë°˜ ë¶„ì‚° ì¶”ë¡  í”„ë ˆì„ì›Œí¬ 

ğŸš€ Distributed Inference Framework with Ray & vLLM for RAG-based LLM Serving


ğŸ“Œ í”„ë¡œì íŠ¸ ê°œìš”
------------------------------------------------------------
ë³¸ í”„ë¡œì íŠ¸ëŠ” Ray ê¸°ë°˜ í´ëŸ¬ìŠ¤í„° ìì› ìŠ¤ì¼€ì¤„ë§ê³¼
vLLMì˜ Paged Attention ê¸°ë°˜ KV ìºì‹œ ìµœì í™”ë¥¼ ê²°í•©í•˜ì—¬,

âœ… ëŒ€ê·œëª¨ LLM + RAG ì„œë¹„ìŠ¤ì˜
âœ… ì§€ì—°ì‹œê°„(TTFT, Latency) ê°ì†Œ
âœ… ì²˜ë¦¬ëŸ‰(TPS) ê·¹ëŒ€í™”

ë¥¼ ëª©ì ìœ¼ë¡œ í•œë‹¤.


ğŸ“Œ ë¹„êµ í‰ê°€ ëŒ€ìƒ ë¶„ì‚° êµ¬ì¡° (4ê°€ì§€)
------------------------------------------------------------
â‘  Single Node (Baseline)
â‘¡ Ray Actor Cluster (RAC)
â‘¢ Ray Serve Cluster (RSC)
â‘£ Ray + vLLM Cluster (RVC)

ë³¸ í”„ë¡œì íŠ¸ëŠ” ë‹¤ìŒ ë…¼ë¬¸ì˜ ì‹¤í—˜ êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œë‹¤.

ğŸ“„ Performance Analysis of a Rayâ€“vLLM Based Distributed System for Efficient LLM Inference  
ğŸ› USTâ€“ETRI, 2025



ğŸ“‘ TABLE OF CONTENTS
============================================================
1. ì „ì²´ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜
2. êµ¬ì„± ìš”ì†Œ
3. ì„±ëŠ¥ í‰ê°€ ì§€í‘œ (KPM)
4. ì‹¤í—˜ ì½”ë“œ êµ¬ì„±
5. ì‹¤í–‰ í™˜ê²½
6. ì‹¤í–‰ ë°©ë²•
7. vLLM íŠœë‹ ì˜µì…˜
8. ë¶„ì‚° êµ¬ì¡°ë³„ í•µì‹¬ ì°¨ì´
9. í•µì‹¬ ì‹¤í—˜ ê²°ê³¼ ìš”ì•½
10. í”„ë¡œì íŠ¸ ëª©ì  ìš”ì•½
11. í–¥í›„ í™•ì¥
12. Citation



âœ… 1. ì „ì²´ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜
============================================================

<br>
[ User Query ] <br>
â†“ <br>
[ RAG Pipeline (FAISS + ko-sroberta) ] <br>
â†“ <br>
[ Prompt Expansion ] <br>
â†“ <br>
[ Distributed Inference ] <br>
â†“ <br>
[ LLM Output ] <br>
<br>
<br>



âœ… 2. êµ¬ì„± ìš”ì†Œ
============================================================

ğŸ”¹ RAG               : FAISS + ko-sroberta-multitask  
ğŸ”¹ LLM               : Qwen2.5-3B, Qwen2.5-1.5B-Instruct  
ğŸ”¹ Distributed Runtime: Ray (Multi-node, Multi-GPU)  
ğŸ”¹ Inference Optimizer: vLLM (Paged Attention, Continuous Batching)



âœ… 3. ì„±ëŠ¥ í‰ê°€ ì§€í‘œ (KPM)
============================================================

<br>
ì§€í‘œ | ì„¤ëª… <br>
TTFT | Time To First Token <br>
Latency_avg | ìš”ì²­ 1ê°œë‹¹ í‰ê·  ì²˜ë¦¬ ì§€ì—° <br>
Latency_total | ë¼ìš´ë“œ ì „ì²´ ë²½ì‹œê³„ ì§€ì—° <br>
TPS | ì´ˆë‹¹ ì²˜ë¦¬ í† í° ìˆ˜ <br>
<br>
â€» ëª¨ë“  ì‹¤í—˜ì€ <br>
âœ” ë™ì¼í•œ ì§ˆë¬¸ <br>
âœ” ë™ì¼í•œ RPS <br>
âœ” ë™ì¼í•œ ëª¨ë¸ <br>
ì¡°ê±´ì—ì„œ ìˆ˜í–‰ëœë‹¤. <br>
<br>
<br>



âœ… 4. ì‹¤í—˜ ì½”ë“œ êµ¬ì„±
============================================================

<br>
baseline.py : ë‹¨ì¼ ë…¸ë“œ ë©€í‹°ìŠ¤ë ˆë“œ ê¸°ë°˜ ì¶”ë¡  <br>
baseline_fetched.py : Baseline ê°œì„ íŒ <br>
baseline_quality_check.py : Baseline ì¶”ë¡  ê²°ê³¼ í’ˆì§ˆ ê²€ì¦ <br>
RayCluster_fetched.py : Ray Actor ê¸°ë°˜ ë¶„ì‚° ì¶”ë¡  <br>
RayServe_fetched.py : Ray Serve ê¸°ë°˜ ì„œë¹™ êµ¬ì¡° <br>
RayVllm_fetched.py : Ray + vLLM í†µí•© ë¶„ì‚° ì¶”ë¡  <br>
<br>
<br>



âœ… 5. ì‹¤í–‰ í™˜ê²½
============================================================

<br>
[5.1] í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ <br>
pip install torch transformers langchain faiss-cpu ray vllm <br>
<br>
[5.2] ê³µí†µ í™˜ê²½ ë³€ìˆ˜ (ì˜¤í”„ë¼ì¸ ëª¨ë“œ) <br>
export HF_HOME=/mnt/shared/hf-home <br>
export HF_HUB_OFFLINE=1 <br>
export TRANSFORMERS_OFFLINE=1 <br>
export HF_DATASETS_OFFLINE=1 <br>
<br>
[5.3] ë””ë ‰í† ë¦¬ êµ¬ì¡° <br>
/mnt/shared <br>

faiss_index <br>

models <br>

ko-sroberta-multitask <br>

qwen25_3b <br>

qwen25_1_5b_instruct <br>

<br>
<br>


âœ… 6. ì‹¤í–‰ ë°©ë²•
============================================================

<br>
[6.1] Baseline (Single Node) <br>
python baseline.py <br>
ë˜ëŠ” <br>
python baseline_fetched.py <br>
<br>
[6.2] Ray Actor Cluster (RAC) <br>
ray start --head <br>
ray start --address=<HEAD_NODE_IP>:6379 <br>
python RayCluster_fetched.py <br>
<br>
[6.3] Ray Serve Cluster (RSC) <br>
ray start --head <br>
python RayServe_fetched.py <br>
<br>
[6.4] Ray + vLLM Cluster (RVC) <br>
ray start --head <br>
python RayVllm_fetched.py <br>
<br>
<br>


âœ… 7. vLLM íŠœë‹ ì˜µì…˜
============================================================

export VLLM_MAX_MODEL_LEN=4096  
export VLLM_GPU_MEM_UTIL=0.69  
export VLLM_KV_CACHE_DTYPE=auto  
export VLLM_QUANT=awq  



âœ… 8. ë¶„ì‚° êµ¬ì¡°ë³„ í•µì‹¬ ì°¨ì´
============================================================

Baseline : ë‹¨ì¼ ë…¸ë“œ, Python Thread ê¸°ë°˜ <br>
Ray Actor : GPU 1ê°œë‹¹ Actor 1ê°œ, ëª…ì‹œì  ìŠ¤ì¼€ì¤„ë§ <br>
Ray Serve : HTTP ê¸°ë°˜ ì„œë¹™, Replica ë¶„ì‚° <br>
Ray + vLLM : PagedAttention + ì—°ì† ë°°ì¹­/KVìºì‹œ ìµœì í™” <br>
<br>
<br>

âœ… 9. í•µì‹¬ ì‹¤í—˜ ê²°



âœ… 9. í•µì‹¬ ì‹¤í—˜ ê²°ê³¼ ìš”ì•½ (ë…¼ë¬¸ ê¸°ì¤€)
============================================================

TTFT : ğŸ”» 88.49% ê°ì†Œ <br>
í‰ê·  Latency : ğŸ”» 72.99% ê°ì†Œ <br>
ì „ì²´ Latency : ğŸ”» 70.97% ê°ì†Œ <br>
TPS : ğŸ”º 171.18% ì¦ê°€ <br>
<br>
âœ” ê³ ë¶€í•˜(RPS â‰¥ 24) í™˜ê²½ì—ì„œ <br>
âœ” ì—°ì† ë°°ì¹­ + KV ìºì‹œ í˜ì´ì§• íš¨ê³¼ ê·¹ëŒ€í™” <br>
âœ” ë©”ëª¨ë¦¬ ë‹¨í¸í™” ê°ì†Œ ë° GPU ìœ íœ´ ì‹œê°„ ìµœì†Œí™” <br>
<br>
<br>


âœ… 10. í”„ë¡œì íŠ¸ ëª©ì  ìš”ì•½
============================================================

RayëŠ” ğŸ”§ í´ëŸ¬ìŠ¤í„° ì „ì²´ ìì› ìŠ¤ì¼€ì¤„ë§ ë‹´ë‹¹  
vLLMì€ âš¡ ë…¸ë“œ ë‚´ë¶€ ì¶”ë¡  ê·¹í•œ ìµœì í™” ë‹´ë‹¹  

â¡ ì´ ë‘˜ì˜ ê²°í•©ì´
â¡ í˜„ì¬ êµ¬ì¡° ì¤‘ ê°€ì¥ ê°•ë ¥í•œ
â¡ LLM ë¶„ì‚° ì„œë¹™ í•´ë²•ì„ì„ ì‹¤ì¦í•œë‹¤.



âœ… 11. í–¥í›„ í™•ì¥
============================================================

- ğŸ“¦ Docker ê¸°ë°˜ Ray + vLLM ë°°í¬ ìŠ¤íƒ
- ğŸ”Œ gRPC ê¸°ë°˜ ì‹¤ì‹œê°„ ì„œë¹™ API
- ğŸ”€ Multi-LLM íŒŒì´í”„ë¼ì¸ ìë™ ë¶„ì‚° ìŠ¤ì¼€ì¤„ë§
- ğŸ§  NPU ê¸°ë°˜ vLLM ëŸ°íƒ€ì„ ì´ì‹



âœ… 12. Citation
============================================================

@article{kang2025rayvllm,
  title  = {Performance Analysis of a Rayâ€“vLLM Based Distributed System for Efficient LLM Inference},
  author = {Kang, Minsu and Kim, Young-Joo and Kim, Seon-Tae},
  journal= {UST-ETRI},
  year   = {2025}
}


