Rayâ€“vLLM ê¸°ë°˜ ë¶„ì‚° ì¶”ë¡  í”„ë ˆì„ì›Œí¬
Distributed Inference Framework with Ray & vLLM for RAG-based LLM Serving

ë³¸ í”„ë¡œì íŠ¸ëŠ” Ray ê¸°ë°˜ í´ëŸ¬ìŠ¤í„° ìì› ìŠ¤ì¼€ì¤„ë§ê³¼
vLLMì˜ Paged Attention ê¸°ë°˜ KV ìºì‹œ ìµœì í™”ë¥¼ ê²°í•©í•˜ì—¬
ëŒ€ê·œëª¨ LLM + RAG ì„œë¹„ìŠ¤ì˜ ì§€ì—°ì‹œê°„(TTFT, Latency) ê°ì†Œ ë° ì²˜ë¦¬ëŸ‰(TPS) ê·¹ëŒ€í™”ë¥¼ ëª©ì ìœ¼ë¡œ í•œë‹¤.

ë™ì¼í•œ í•˜ë“œì›¨ì–´ í™˜ê²½ì—ì„œ ë‹¤ìŒ ë„¤ ê°€ì§€ êµ¬ì¡°ë¥¼ ì •ëŸ‰ ë¹„êµ í‰ê°€í•œë‹¤.

Single Node (Baseline)

Ray Actor Cluster (RAC)

Ray Serve Cluster (RSC)

Ray + vLLM Cluster (RVC)

ë³¸ í”„ë¡œì íŠ¸ëŠ” ë‹¤ìŒ ë…¼ë¬¸ì˜ ì‹¤í—˜ êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œë‹¤.

Performance Analysis of a Rayâ€“vLLM Based Distributed System for Efficient LLM Inference
USTâ€“ETRI, 2025

ğŸ“‘ Table of Contents

1. ì „ì²´ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

2. êµ¬ì„± ìš”ì†Œ

3. ì„±ëŠ¥ í‰ê°€ ì§€í‘œ (KPM)

4. ì‹¤í—˜ ì½”ë“œ êµ¬ì„±

5. ì‹¤í–‰ í™˜ê²½

6. ì‹¤í–‰ ë°©ë²•

7. vLLM íŠœë‹ ì˜µì…˜

8. ë¶„ì‚° êµ¬ì¡°ë³„ í•µì‹¬ ì°¨ì´

9. í•µì‹¬ ì‹¤í—˜ ê²°ê³¼ ìš”ì•½

10. í”„ë¡œì íŠ¸ ëª©ì  ìš”ì•½

11. í–¥í›„ í™•ì¥

12. Citation

1. ì „ì²´ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜
User Query
   â†“
RAG Pipeline (FAISS + ko-sroberta)
   â†“
Prompt Expansion
   â†“
Distributed Inference
   â†“
LLM Output

2. êµ¬ì„± ìš”ì†Œ

RAG: FAISS + ko-sroberta-multitask

LLM: Qwen2.5-3B, Qwen2.5-1.5B-Instruct

Distributed Runtime: Ray (Multi-node, Multi-GPU)

Inference Optimizer: vLLM (Paged Attention, Continuous Batching)

3. ì„±ëŠ¥ í‰ê°€ ì§€í‘œ (KPM)
ì§€í‘œ	ì„¤ëª…
TTFT	Time To First Token
Latency_avg	ìš”ì²­ 1ê°œë‹¹ í‰ê·  ì²˜ë¦¬ ì§€ì—°
Latency_total	ë¼ìš´ë“œ ì „ì²´ ë²½ì‹œê³„ ì§€ì—°
TPS	ì´ˆë‹¹ ì²˜ë¦¬ í† í° ìˆ˜

ëª¨ë“  ì‹¤í—˜ì€ ë™ì¼í•œ ì§ˆë¬¸, ë™ì¼í•œ RPS, ë™ì¼í•œ ëª¨ë¸ ì¡°ê±´ì—ì„œ ìˆ˜í–‰ëœë‹¤.

4. ì‹¤í—˜ ì½”ë“œ êµ¬ì„±
íŒŒì¼ëª…	ì„¤ëª…
baseline.py	ë‹¨ì¼ ë…¸ë“œ ë©€í‹°ìŠ¤ë ˆë“œ ê¸°ë°˜ ì¶”ë¡ 
baseline_fetched.py	Baseline ê°œì„ íŒ (n=1ì—ì„œ Q1+Q2 ë™ì‹œ ì²˜ë¦¬)
baseline_quality_check.py	Baseline ì¶”ë¡  ê²°ê³¼ í’ˆì§ˆ ê²€ì¦
RayCluster_fetched.py	Ray Actor ê¸°ë°˜ ë¶„ì‚° ì¶”ë¡ 
RayServe_fetched.py	Ray Serve ê¸°ë°˜ ì„œë¹™ êµ¬ì¡°
RayVllm_fetched.py	Ray + vLLM í†µí•© ë¶„ì‚° ì¶”ë¡  êµ¬ì¡° (ìµœì¢… ëª¨ë¸)
5. ì‹¤í–‰ í™˜ê²½
5.1 í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
pip install torch transformers langchain faiss-cpu ray vllm

5.2 ê³µí†µ í™˜ê²½ ë³€ìˆ˜ (ì˜¤í”„ë¼ì¸ ëª¨ë“œ)
export HF_HOME=/mnt/shared/hf-home
export HF_HUB_OFFLINE=1
export TRANSFORMERS_OFFLINE=1
export HF_DATASETS_OFFLINE=1

5.3 ë””ë ‰í† ë¦¬ êµ¬ì¡°
/mnt/shared
 â”œâ”€â”€ faiss_index
 â””â”€â”€ models
     â”œâ”€â”€ ko-sroberta-multitask
     â”œâ”€â”€ qwen25_3b
     â””â”€â”€ qwen25_1_5b_instruct

6. ì‹¤í–‰ ë°©ë²•
6.1 Baseline (Single Node)
python baseline.py


ë˜ëŠ” ê°œì„ íŒ:

python baseline_fetched.py

6.2 Ray Actor Cluster (RAC)
ray start --head
ray start --address=<HEAD_NODE_IP>:6379
python RayCluster_fetched.py

6.3 Ray Serve Cluster (RSC)
ray start --head
python RayServe_fetched.py

6.4 Ray + vLLM Cluster (RVC, ìµœì¢… êµ¬ì¡°)
ray start --head
python RayVllm_fetched.py

7. vLLM íŠœë‹ ì˜µì…˜
export VLLM_MAX_MODEL_LEN=4096
export VLLM_GPU_MEM_UTIL=0.69
export VLLM_KV_CACHE_DTYPE=auto
export VLLM_QUANT=awq

8. ë¶„ì‚° êµ¬ì¡°ë³„ í•µì‹¬ ì°¨ì´
êµ¬ì¡°	íŠ¹ì§•
Baseline	ë‹¨ì¼ ë…¸ë“œ, Python Thread ê¸°ë°˜
Ray Actor	GPU 1ê°œë‹¹ Actor 1ê°œ, ëª…ì‹œì  ìŠ¤ì¼€ì¤„ë§
Ray Serve	HTTP ê¸°ë°˜ ì„œë¹™, Replica ë¶„ì‚°
Ray + vLLM	PagedAttention + Continuous Batching + KV Cache ìµœì í™”
9. í•µì‹¬ ì‹¤í—˜ ê²°ê³¼ ìš”ì•½ (ë…¼ë¬¸ ê¸°ì¤€)

Ray + vLLM êµ¬ì¡°(RVC)ëŠ” Ray Actor ëŒ€ë¹„ ë‹¤ìŒ ì„±ëŠ¥ í–¥ìƒì„ ë‹¬ì„±í•˜ì˜€ë‹¤.

ì§€í‘œ	ê°œì„ ë¥ 
TTFT	88.49% ê°ì†Œ
í‰ê·  Latency	72.99% ê°ì†Œ
ì „ì²´ Latency	70.97% ê°ì†Œ
TPS	171.18% ì¦ê°€

ê³ ë¶€í•˜(RPS â‰¥ 24) í™˜ê²½ì—ì„œ ì—°ì† ë°°ì¹­ + KV ìºì‹œ í˜ì´ì§• íš¨ê³¼ ê·¹ëŒ€í™”

ë©”ëª¨ë¦¬ ë‹¨í¸í™” ê°ì†Œ ë° GPU ìœ íœ´ ì‹œê°„ ìµœì†Œí™”

10. í”„ë¡œì íŠ¸ ëª©ì  ìš”ì•½

RayëŠ” í´ëŸ¬ìŠ¤í„° ì „ì²´ ìì› ìŠ¤ì¼€ì¤„ë§ì„ ë‹´ë‹¹í•˜ê³ ,
vLLMì€ ë…¸ë“œ ë‚´ë¶€ ì¶”ë¡ ì„ ê·¹ë‹¨ì ìœ¼ë¡œ ìµœì í™”í•œë‹¤.

ì´ ë‘˜ì˜ ê²°í•©ì´ í˜„ì¬ êµ¬ì¡° ì¤‘
ê°€ì¥ ê°•ë ¥í•œ LLM ë¶„ì‚° ì„œë¹™ í•´ë²•ì„ì„ ì‹¤ì¦í•œë‹¤.

11. í–¥í›„ í™•ì¥

Docker ê¸°ë°˜ Ray + vLLM ë°°í¬ ìŠ¤íƒ

gRPC ê¸°ë°˜ ì‹¤ì‹œê°„ ì„œë¹™ API

Multi-LLM íŒŒì´í”„ë¼ì¸ ìë™ ë¶„ì‚° ìŠ¤ì¼€ì¤„ë§

NPU ê¸°ë°˜ vLLM ëŸ°íƒ€ì„ ì´ì‹

12. Citation
@article{kang2025rayvllm,
  title={Performance Analysis of a Rayâ€“vLLM Based Distributed System for Efficient LLM Inference},
  author={Kang, Minsu and Kim, Young-Joo and Kim, Seon-Tae},
  journal={UST-ETRI},
  year={2025}
}
